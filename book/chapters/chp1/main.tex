% Write the full path to the location of the graphics relative to book.tex
\graphicspath{{chapters/chp1/graphics/}}

\title{A CUDA extension for FEniCSx: CUDOLFINx}
\titlerunning{A CUDA extension for FEniCSx: CUDOLFINx}

\author{Benjamin.~A.~Pachev and James.~D.~Trotter}
\authorrunning{Pachev et al.}
\institute{Benjamin.~A.~Pachev \email{benjmainpachev@utexas.edu} \at The University of Texas at Austin \\James.~D.~Trotter \email{james@simula.no} \at Simula Research Laboratory}


\maketitle

\abstract{Here we introduce CUDOLFINx - a Python package which extends FEniCSx with GPU accelerated assembly capabilities. The extension enables FEniCSx codes to be accelerated on the GPU with minimal changes, and provides an easy path for researchers to experiment with GPU-accelerated PDE solvers. By constrast with previous efforts to enhance FEniCSx with GPU capabilities, CUDOLFINX is designed as a standalone package and does not require major changes to the core components of FEniCSx. Consequently, it has the potential to become a usable part of the FEniCSx ecosystem and a long-term solution to the problem of providing GPU acceleration capabilities in FEniCSx.
We further present performance benchmarks for a representative range of GPU-accelerated FEniCSx applications on the next-generation NVIDIA GH200 Superchip. Our results indicate that CUDOLFINx provides significant speedups compared to traditional CPU parallelization in DOLFINx for a variety of problems. 
}

\section*{Introduction}

Graphics processing units (GPUs) provide an alternative means of parallelizing computations to traditional parallelism across clusters of multi-core CPUs. For many applications, GPUs are more energy-efficient and have revolutionized fields such as machine learning \cite{navarro2014survey}. However, the application of GPU acceleration to PDE solvers has yet to become the norm \cite{fu2014architecting}. A major reason for this is the difficulty of modifying existing codes to use GPU parallelism \cite{MILLS2021102831}. Furthermore, parallelization schemes suitable for clusters of multi-core CPUs frequently don't work for GPUs \cite{MACIOL20101093}. Maximizing GPU performance often requires a customized algorithm, which further increases the difficulty of integrating GPU acceleration into existing projects \cite{mittal2015survey}. Despite these challenges, a large number of software packages used for solving PDEs have taken steps to provide GPU acceleration capabilities. These include PETSc \cite{MILLS2021102831}, MFEM \cite{anderson2021mfem}, libCEED \cite{abdelfattah2021gpu}, and deal.II \cite{arndt2021deal}. Significant efforts have been made to develop GPU-accelerated linear solvers, including in libraries such as Ginkgo \cite{ginkgo-toms-2022}, AMGx \cite{naumov2015amgx}, hypre \cite{li2020efficient, falgout2021porting}, SuperLU \cite{li2023newly}, and others \cite{lu2023tilesptrsv}.

A major attraction of FEniCSx \cite{baratta2023dolfinx} as a tool for solving partial differential equations with the finite element method is its simple Python interface and automated generation of efficient C code. These features enable rapid prototyping and development of performant solvers for complicated PDEs. Our goal in developing CUDOLFINx is to enable users of FEniCSx to add GPU acceleration to their existing PDE solvers with minimal effort, similar to how FEniCSx automatically handles MPI parallelization. During the remainder of this chapter, we will give a brief summary of the architecture of CUDOLFINx, present some perfomance benchmarks for the Poisson, Navier-Stokes, and shallow water equations, and finally discuss future development of CUDOLFINx. 

\section*{Architecture of CUDOLFINx}

As the majority of users of FEniCSx rely on the Python interface, CUDOLFINx is primarily targeted towards Python users. It adds a \textit{CUDAAssembler} class whose methods replace assembly routines of the same name in DOLFINx. These include:
\begin{itemize}
\item \textit{apply\_lifting}
\item \textit{assemble\_matrix}
\item \textit{assemble\_vector}
\item \textit{create\_matrix}
\item \textit{create\_vector}
\item \textit{pack\_bcs}
\item \textit{set\_bc}
\end{itemize}
Additionally, the \textit{fem.form} function is replaced with a \textit{cudolfinx.form} function, which returns a special \textit{CUDAForm} class that can be used for GPU assembly. The extension additionally relies on the CUDA-enabled version of PETSc for GPU linear algebra routines. The CUDOLFINx Github repository contains installation instructions and example usage (\href{https://github.com/bpachev/dolfinx\_ipcs}{github.com/bpachev/dolfinx\_ipcs}).

The GPU acceleration scheme within CUDOLFINx is an extension of the work of \cite{trotter2023targeting}. The generated tabulate tensor routines from FFCx are used as-is, with no GPU-targeted changes. Consequently, each element is processed by a single GPU thread. The resulting element tabulate tensors are assembled into a global matrix with atomic operations to prevent data races. This approach works well for low-order elements, but is not appropriate for high-order methods, as these necessitate the use of multiple GPU threads per element to obtain an efficient algorithm \cite{MACIOL20101093,dziekonski2013generation,banas2014numerical}. Nevertheless, low order methods constitute a large fraction of usecases for FEniCSx, and so our current methodology has the potential to benefit these applications. We will proceed to demonstrate the peformance of CUDOLFINx for several representative use cases in which GPU acceleration can significantly enhance FEniCS code.

\section*{Performance Benchmarks}

We begin with two examples of the performance of the GPU assembly kernels in CUDOLFINx, and then present a use case of complete GPU offloading for both assembly and linear solves.

\subsection*{Poisson}

%The classical introductory PDE is the Poisson equation, given by:
%\begin{align}
%  - \nabla^{2} u &= f \quad {\rm in} \ \Omega, \\
%  u &= 0 \quad {\rm on} \ \Gamma_{D}, \\
%  \nabla u \cdot n &= g \quad {\rm on} \ \Gamma_{N}.
%\end{align}
%Here $\Omega$ is the solution domain, $\Gamma_{D}$ is the Dirichlet boundary, and $\Gamma_{N}$ the Neumann boundary, with $f$ and $g$ some functions. Integration by parts leads to the variational problem:
%\begin{equation}
%\int_{\Omega}\nabla u \cdot \nabla v dx = \int_{\Omega}fv dx + \int_{\Gamma_n} gv ds,
%\end{equation}
%with $v$ a test function assumed to be zero on $\Gamma_D$. Solving the variational problem requires the assembly of a stiffness matrix on the left side, as well as a load vector on the right side. The dominant step in assembly is the stiffness matrix. 

Here we consider the problem of assembling a stiffness matrix for the solution of the Poisson equation on the unit cube. We use an order-one Lagrange finite element space, with tetrahedral elements and a uniform discretization. Table \ref{tab:meshes} shows the meshes used in this and the subsequent studies. We compared the performance of the CUDOLFINx assembly on three uniform meshes for the Poisson problem on an NVIDIA GH200 GPU to DOLFINx assembly using 72 MPI process on an NVIDIA Grace CPU. For each mesh, we report the throughput in millions of degrees of freedom per second. The results are summarized in Table \ref{tab:poisson_results}.
\begin{table}[t]
    \centering
\begin{tabular}{lrrr}
\toprule
Mesh & Usecase & Cells & Vertices\\
\midrule
Uniform 1 & Poisson & 6,000,000 & 1,030,301 \\
Uniform 2 & Poisson & 16,464,000 & 2,803,221 \\
Uniform 3 & Poisson & 48,000,000 & 8,120,601 \\
Tidal 1 & Shallow Water & 980,000 & 491,401 \\
Tidal 2 & Shallow Water & 2,000,000 & 1,002,001 \\
Tidal 3 & Shallow Water & 3,380,000 & 1,692,601 \\
channel3D-lc003 & Navier-Stokes & 1,995,628 & 355,319 \\
\bottomrule
\end{tabular}
\caption{List of meshes.}
    \label{tab:meshes}
\end{table}

\begin{table}[t]
    \centering
\begin{tabular}{lrr}
\toprule
Mesh & GH200 GPU & Grace CPU \\
\midrule
Uniform 1 & 134.81 & 9.13 \\ 
Uniform 2 & 134.42 & 8.94 \\ 
Uniform 3 & 136.88 & 9.59 \\ 
\bottomrule
\end{tabular}
\caption{Performance of Poisson matrix assembly, in millions of dofs per second.}
    \label{tab:poisson_results}
\end{table}

\subsection*{Shallow Water Equations}

While the Poisson equations are a useful testscase, they don't represent the full range of complexity enabled by FEniCSx. For a more realistic example, we present some assembly benchmarks using the variational forms in SWEMniCS \cite{dawson2024swemnics}, a FEniCSx-based solver for the shallow water equations. SWEMniCS implements a suite of stablized two-dimensional shallow water solvers with implicit time stepping. Due to the nonlinearity of the shallow water equations, a Newton solve is required at each time step. We investigated the efficiency of automatically generated CUDA assembly kernels for two stabilized schemes within SWEMniCS: the Discontinuous Galerkin (DG), and the Streamline Upwind Petrov-Galerkin (SUPG) schemes. The DG method uses broken test and trial spaces with a Lax-Freidrichs numerical flux. On the other hand, the SUPG scheme uses continuous spaces, but adds a fairly complicated stabilization term to the variational form. As a continuous method, SUPG has fewer degrees of freedom, but is less numerically stable than DG. For each scheme, we averaged the performance of both GPU and CPU assembly kernels over twenty Newton iterations for tidal flow simulations on a set of square uniform triangular meshes. Results are presented for both assembly of the Jacobian matrix and the residual vector. In this experiment, an NVIDIA A100 GPU was compared to a 64-core AMD Epyc CPU.
Table \ref{tab:swe_a100_vs_epyc} summarizes the results of the experiment.
\begin{table}[t]
    \centering
    \begin{tabular}{lrrll}
\toprule
Mesh & DG Jacobian & DG Residual & SUPG Jacobian & SUPG Residual \\
\midrule
Tidal 1 & 0.98 & 9.21 & 5.36 & 6.78 \\
Tidal 2 & 0.99 & 12.18 & 5.51 & 6.84 \\
Tidal 3 & 0.95 & 9.66 & 5.36 & 5.60 \\
\bottomrule
\end{tabular}
    \caption{Speedups of assembly kernels on an NVIDIA A100 GPU relative to a 64-core AMD Epyc CPU. Larger numbers indicate faster GPU runtimes.}
    \label{tab:swe_a100_vs_epyc}
\end{table}

Interestingly, the speedups differ for each variational form. The DG residual vector assembly is extremely fast, while the Jacobian matrix assembly is less performant. On the other hand, SUPG has consistent speedups.
Using NVIDIA's Nsight Compute profiler, we can better understand the difference in performance between DG and SUPG.
Profiler results for the 'Tidal 3' testcase are presented in Table \ref{tab:tidal_prof}. The profiler reports \textit{occupancy}, which indicates the percentage of active threads on the GPU relative to the total GPU thread capacity. Occupancy can be limited by the resources needed per thread, such as shared memory or registers. It can also be impacted by excessive branching or other kernel design flaws. In our case, the generated assembly kernels required a high number of registers, which limited the occupancy to under 20\%. However, both the DG and SUPG kernels were able to utilize a large fraction of the device throughput - memory in the case of DG and compute in the case of SUPG. 
\begin{table}[t]
    \centering
\begin{tabular}{rrrrrr}
\toprule
Method & Kernel & \begin{tabular}{@{}l}Theoretical\\ Occupancy\end{tabular} & \begin{tabular}{@{}l}Achieved\\ Occupancy\end{tabular} & \begin{tabular}{@{}l}Memory\\ Throughput\end{tabular} & \begin{tabular}{@{}l}Compute\\ Throughput\end{tabular} \\
\midrule
DG & jacobian & 12.50\,\% & 12.33\% & 41.44\,\% & 10.43\,\% \\
SUPG & jacobian & 12.50\,\% & 12.33\,\% & 48.66\,\% & 59.31\,\% \\
DG & residual & 18.75\,\% & 18.74\,\% & 64.97\,\% & 18.20\,\% \\
SUPG & residual & 12.50\,\% & 12.39\,\% & 5.64\,\% & 77.85\,\% \\
\bottomrule
\end{tabular}
\caption{Profiling statistics for the GPU assembly kernels on a square mesh.}
    \label{tab:tidal_prof}
\end{table}

The shallow water results were obtained with a DOLFINx fork that was a precursor to CUDOLFINx, but used the same GPU assembly strategy. The code consequently isn't currently part of the CUDOLFINx examples on Github, but can be made avaialable upon request.

\subsection*{Navier Stokes}

While matrix assembly is a crucial part of the finite element method, most solvers also require the solution of linear systems. To assess the practicality of offloading a typical FEniCSx code to the GPU, we enhanced an existing FEniCSx Navier-Stokes solver with our extension. The original code can be found at \href{https://github.com/jorgensd/dolfinx\_ipcs}{github.com/jorgensd/dolfinx\_ipcs}. Our CUDA-accelerated version is available at \href{https://github.com/bpachev/dolfinx\_ipcs}{github.com/bpachev/dolfinx\_ipcs}. The original solver solves the Navier-Stokes equations with a BDF2 method, and requires three stages per time step. While each stage requires a linear solve and assembly of a vector, only the first requires reassembly of a stiffness matrix. Second-order Lagrange tetrahedral elements were used for the velocity field, and first-order Lagrange elements for the pressure field. The mesh was a refined version of the 3D channel with an obstacle used in the original code, and contained 1,995,628 tetrahedra with 355,319 vertices.

We compared the runtimes of the original solver on a 72-core Grace CPU to the CUDA-accelerated version using a GH200 GPU. Average timing results over 100 time steps for each component of the solver are provided in Table \ref{tab:navier_stokes_results}.
\begin{table}[t]
    \centering
\begin{tabular}{lrrrrr}
\toprule
Stage & Solver/PC & GPU Assembly & GPU Solve & CPU Assembly & CPU Solve \\
\midrule
1 & BCGS/JACOBI & 0.348 & 1.601 & 0.906 & 6.339 \\ 
2 & GMRES/BoomerAMG & 0.009 & 0.013 & 0.007 & 0.108 \\ 
3 & CG/JACOBI & 0.004 & 0.079 & 0.013 & 0.564 \\ 
\bottomrule
\end{tabular}
\caption{Runtimes in seconds for each stage of the Navier-Stokes solver, averaged over 100 time steps.}
    \label{tab:navier_stokes_results}
\end{table}
Overall, the CUDA-accelerated solver averaged 2.07s per time step, while the original solver took 7.89s per time step. While the solution time was dominated by the linear solve for the first stage, assembly comprised over 10\% of the total runtime for both the GPU and CPU codes. We note that compared to the Poisson example, the assembly speedup for the Navier-Stokes code is smaller. This is due to the use of second-order elements for the velocity field. Higher order elements pose difficulties for the GPU offloading approach used within CUDOLFINx because the generated tabulate tensor kernels require a large number of registers. This results in a phenomenon known as \textit{register spilling}, in which the GPU kernels are forced to utilize global memory to store some local variables. This significantly degrades performance. Solutions include retooling of the generated code to require less constant memory, or assembling each element with multiple GPU threads to increase the available number of registers. Both of these solutions would require substantial enhancements to FFCX, the form compiler within the FEniCS project. Nevertheless, the attained speedup for assembly with quadratic elements in this case is still significant.

\section*{Conclusion}

We have introduced CUDOLFINx - an add-on enhancement to FEniCSx which provides GPU-acclerated assembly routines. We've demonstrated that offloading assembly to a GPU can provide significant speedups compared to multi-core CPUs, particularly for first-order elements. The package has a simple Python interface, and can be utilized in complicated FEniCS codebases with little effort. Integration with GPU-accelerated linear solvers in PETSc is easy, and can result in significant end-to-end speedups of FEniCSx finite element workflows.

In the near future, we intend to add support for multiple GPUs. Subsequent efforts will focus on providing support for alterntive GPUs, as well as developing efficient kernels for assembly with high-order elements. Work is currently underway to expand the documentation and examples, as well as to create easily installable binary distributions.

%Sample references \cite{alnaes2015fenics, baratta2023dolfinx}.
%Source code for this book is found at \href{https://github.com/meg-simula/2024-fenics-proceedings}{github.com/meg-simula/2024-fenics-proceedings}


\begin{acknowledgement}
  We gratefully acknolwedge the use of the Lonestar6 and Vista systems at the Texas Advanced Computing Center under the "ADCIRC" allocation.        
\end{acknowledgement}

\bibliographystyle{spbasic}
% Write the full path of your bibfile relative to book.tex
\bibliography{chapters/chp1/bibliography.bib}


